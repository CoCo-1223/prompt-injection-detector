"""
Google Gemini API í´ë¼ì´ì–¸íŠ¸
ì·¨ì•½í•œ ë²„ì „ vs ë³´í˜¸ëœ ë²„ì „
"""

try:
    import google.generativeai as genai
except ImportError:
    print("âš ï¸ google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("pip install google-generativeai ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    raise

from config import Config
from core.detector import PromptInjectionDetector

# defense ëª¨ë“ˆ importë¥¼ í•¨ìˆ˜ ë‚´ë¶€ë¡œ ì´ë™í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
def get_defense_layers():
    """ë°©ì–´ ë ˆì´ì–´ ë™ì  import"""
    try:
        from defense.layer1_filter import check_input_filter
        from defense.layer2_restructure import restructure_prompt
        from defense.layer3_output_filter import check_output_filter
        return check_input_filter, restructure_prompt, check_output_filter
    except ImportError as e:
        print(f"âš ï¸ ë°©ì–´ ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")
        raise

from utils.logger import log_detection

# API ì„¤ì •
try:
    genai.configure(api_key=Config.GEMINI_API_KEY)
except Exception as e:
    print(f"âš ï¸ Gemini API ì„¤ì • ì˜¤ë¥˜: {e}")
    print("GEMINI_API_KEYë¥¼ .env íŒŒì¼ì— ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

class LLMClient:
    def __init__(self):
        try:
            # ìµœì‹  Gemini 2.5 Flash ëª¨ë¸ ì‚¬ìš©
            self.model = genai.GenerativeModel('gemini-2.5-flash')
            self.detector = PromptInjectionDetector()
            print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: gemini-2.5-flash")
        except Exception as e:
            print(f"âš ï¸ gemini-2.5-flash ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            print("ëŒ€ì²´ ëª¨ë¸ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            try:
                # ëŒ€ì²´ ëª¨ë¸ 1: gemini-2.0-flash
                self.model = genai.GenerativeModel('gemini-2.0-flash')
                self.detector = PromptInjectionDetector()
                print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: gemini-2.0-flash")
            except Exception as e2:
                print(f"âš ï¸ gemini-2.0-flash ì´ˆê¸°í™” ì˜¤ë¥˜: {e2}")
                try:
                    # ëŒ€ì²´ ëª¨ë¸ 2: gemini-flash-latest
                    self.model = genai.GenerativeModel('gemini-flash-latest')
                    self.detector = PromptInjectionDetector()
                    print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ: gemini-flash-latest")
                except Exception as e3:
                    print(f"âŒ ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e3}")
                    raise
    
    def get_vulnerable_response(self, user_input: str, user_id: str = 'anonymous'):
        """
        ë°©ì–´ ê¸°ëŠ¥ ì—†ëŠ” ì·¨ì•½í•œ LLM í˜¸ì¶œ
        ê³µê²©ì´ ê·¸ëŒ€ë¡œ í†µí•˜ëŠ” ë²„ì „
        """
        # ì·¨ì•½í•œ ë°©ì‹: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì‚¬ìš©ì ì…ë ¥ ì§ì ‘ ì—°ê²°
        prompt = f"{Config.SYSTEM_PROMPT}\n\nì‚¬ìš©ì: {user_input}\në´‡:"
        
        try:
            response = self.model.generate_content(prompt)
            response_text = response.text
            
            # ë¡œê¹… (ì·¨ì•½ ëª¨ë“œ)
            detection_result = self.detector.detect(user_input, user_id)
            log_detection(user_input, detection_result, response_text, is_vulnerable=True, user_id=user_id)
            
            return {
                'success': True,
                'response': response_text,
                'is_blocked': False
            }
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Gemini API ì˜¤ë¥˜ (ì·¨ì•½ ëª¨ë“œ): {error_msg}")
            return {
                'success': False,
                'error': f"API ì˜¤ë¥˜: {error_msg}",
                'is_blocked': False
            }
    
    def get_protected_response(self, user_input: str, defense_config: dict, user_id: str = 'anonymous'):
        """
        3ë‹¨ê³„ ë°©ì–´ê°€ ì ìš©ëœ ì•ˆì „í•œ LLM í˜¸ì¶œ
        """
        # ë°©ì–´ ë ˆì´ì–´ ë™ì  ë¡œë“œ
        check_input_filter, restructure_prompt, check_output_filter = get_defense_layers()
        
        detection_result = self.detector.detect(user_input, user_id)
        
        # Layer 1: ì…ë ¥ í•„í„°ë§
        if defense_config.get('layer1', True):
            rule_result = check_input_filter(user_input)
            if rule_result['is_blocked']:
                response_text = f"ğŸš« **ì°¨ë‹¨ë¨**: {rule_result['reason']}\n\nì´ ìš”ì²­ì€ í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ê³µê²©ìœ¼ë¡œ ì˜ì‹¬ë˜ì–´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
                log_detection(user_input, detection_result, response_text, is_vulnerable=False, user_id=user_id)
                return {
                    'success': True,
                    'response': response_text,
                    'is_blocked': True,
                    'block_reason': rule_result['reason'],
                    'detection': detection_result
                }
        
        # Layer 2: í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
        if defense_config.get('layer2', True):
            safe_prompt = restructure_prompt(user_input)
        else:
            safe_prompt = f"{Config.SYSTEM_PROMPT}\n\nì‚¬ìš©ì: {user_input}\në´‡:"
        
        try:
            response = self.model.generate_content(safe_prompt)
            response_text = response.text
            
            # Layer 3: ì¶œë ¥ í•„í„°ë§
            if defense_config.get('layer3', True):
                is_leaked, filtered_response = check_output_filter(response_text)
                if is_leaked:
                    response_text = filtered_response
            
            # ë¡œê¹… (ë³´í˜¸ ëª¨ë“œ)
            log_detection(user_input, detection_result, response_text, is_vulnerable=False, user_id=user_id)
            
            return {
                'success': True,
                'response': response_text,
                'is_blocked': False,
                'detection': detection_result
            }
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Gemini API ì˜¤ë¥˜ (ë³´í˜¸ ëª¨ë“œ): {error_msg}")
            return {
                'success': False,
                'error': f"API ì˜¤ë¥˜: {error_msg}",
                'is_blocked': False
            }

# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
try:
    llm_client = LLMClient()
except Exception as e:
    print(f"âš ï¸ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    llm_client = None